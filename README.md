# π“‹ XGBoost & LightGBM ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ‹¤ν—

μΊκΈ€ [Santander Customer Satisfaction](https://www.kaggle.com/competitions/santander-customer-satisfaction/data) λ°μ΄ν„°λ¥Ό κΈ°λ°μΌλ΅

ν•μ΄νΌ νλΌλ―Έν„° νλ‹μ„ ν†µν•΄ XGBoost, LightGBM λ¨λΈμ μ„±λ¥μ„ κ°μ„ ν•κ³ , μµμΆ…μ μΌλ΅ ROC AUC μ„±λ¥μ΄ κ°€μ¥ λ†’μ€ ν•μ΄νΌ νλΌλ―Έν„° μ΅°ν•©μ„ λ„μ¶ν•λ” κ²ƒμ„ λ©ν‘λ΅ μ§„ν–‰ν•μ€μµλ‹λ‹¤.

> μ‹¤ν— ν™κ²½: &nbsp;Python, Scikit-learn, XGBoost, LightGBM, Jupyter Notebook

> μ„±λ¥ κΈ°μ¤€: &nbsp;ROC AUC
> 

<br>

#### 0. Baseline μ„±λ¥ ν™•μΈ

```python
XGBClassifier(n_estimators=500, learning_rate=0.05, eval_metric='auc')
```

- μ΄κΈ° ROC AUC: 0.8413
- κΈ°λ³Έ μ„¤μ •λ§μΌλ΅λ„ λ†’μ€ μ„±λ¥μ„ λ³΄μ€μΌλ‚, λ¨λΈ λ³µμ΅λ„ μ μ–΄(max_depth, min_child_weight)μ™€ λ°μ΄ν„° λ¶κ· ν• λ€μ‘μ„ ν†µν• μ„±λ¥ κ°μ„ μ μ—¬μ§€κ°€ μμΌλ―€λ΅ μ¶”κ°€ μ‹¤ν—μ΄ ν•„μ”ν•λ‹¤.

<br>

## Part 1. XGBoost ν•μ΄νΌνλΌλ―Έν„° νλ‹

#### HyperOpt κΈ°λ° νλ‹ κ²°κ³Ό

```python
# Best params from HyperOpt
{
    'n_estimators': 500,
    'colsample_bytree': 0.6849,
    'learning_rate': 0.0709,
    'max_depth': 6,
    'min_child_weight': 5
}
```

- μ΄κΈ° ROC AUC: 0.8465
- μ „λ°μ μΌλ΅ μ•μ •μ μΈ μ„±λ¥μ„ λ³΄μΈλ‹¤. μ΄ μ„¤μ •μ„ κΈ°μ¤€μΌλ΅ μ„Έλ¶€ μ‹¤ν— μ§„ν–‰ν•μ€λ‹¤.

<br>

#### μ‹¤ν— 1. max_depth & min_child_weight

|  | max_depth | min_child_weight | ROC AUC |
| --- | --- | --- | --- |
| 1-1 | 5 | 4 | 0.8441 |
| 1-2 | 5 | 5 | 0.8456 |
| 1-3 | 6 | 5 | **0.8465** |
| 1-4 | 6 | 6 | 0.8455 |
| 1-5 | 7 | 5 | 0.8463 |

- max_depth=6, min_child_weight=5μΌ λ•, ROC-AUCκ°€ 0.8465λ΅ κ°€μ¥ λ†’μ€ μ„±λ¥μ„ λ³΄μΈλ‹¤.
- λ„λ¬΄ λ‚®κ±°λ‚ λ†’μ€ depthλ” μ„±λ¥ μ €ν• λλ” κ³Όμ ν•© λ¬Έμ κ°€ λ°μƒν•  μ μμΌλ―€λ΅ μ£Όμκ°€ ν•„μ”ν•λ‹¤.

<br>

#### μ‹¤ν— 2. colsample_bytree

|  | colsample_bytree | ROC AUC |
| --- | --- | --- |
| 2-1 | 0.60 | 0.8464 |
| 2-2 | 0.65 | **0.8469** |
| 2-3 | 0.70 | 0.8457 |
| 2-4 | 0.80 | 0.8446 |

- colsample_bytree=0.65μ—μ„ ROC-AUCκ°€ 0.8469λ΅ λ―Έμ„Έν• μ„±λ¥ ν–¥μƒμ„ λ³΄μΈλ‹¤.
- λ„λ¬΄ λ†’κ±°λ‚ λ‚®μ€ κ²½μ° λ¨λ‘ μ„±λ¥ μ €ν• κ²½ν–¥μ΄ μμΌλ―€λ΅, μ μ ν• ν”Όμ² μƒν”λ§ λΉ„μ¨μ΄ ν•„μ”ν•λ‹¤.

<br>

#### μ‹¤ν— 3. learning_rate & n_estimators

|  | learning_rate | n_estimators | ROC AUC |
| --- | --- | --- | --- |
| 3-1 | 0.05 | 500 | 0.8468 |
| 3-2 | 0.03 | 700 | 0.8464 |
| 3-3 | 0.06 | 500 | **0.8471** |
| 3-4 | 0.07 | 500 | 0.8462 |

- learning_rate=0.06μ—μ„ ROC-AUCκ°€ 0.8471λ΅ λ―Έμ„Έν• μ„±λ¥ ν–¥μƒμ„ λ³΄μΈλ‹¤.

<br>

#### μµμΆ… ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•© (XGBoost)

```python
XGBClassifier(
    n_estimators=500,
    learning_rate=0.06,
    max_depth=6,
    min_child_weight=5,
    colsample_bytree=0.65,
    early_stopping_rounds=100,
    eval_metric='auc',
    use_label_encoder=False
)
```

- μµμΆ… ROC AUC: 0.8471

<br>
<br>

## Part 2. LightGBM ν•μ΄νΌνλΌλ―Έν„° νλ‹

#### 1. HyperOpt κΈ°λ° νλ‹ κ²°κ³Ό

```python
# Best params from HyperOpt
{
    'n_estimators': 500,
    'learning_rate': 0.0594,
    'max_depth': 115,
    'min_child_samples': 64,
    'num_leaves': 38,
    'subsample': 0.8226
}
```
- μ΄κΈ° ROC AUC: 0.8423

<br>

#### μ‹¤ν— 1. max_depth

|  | max_depth | ROC AUC |
| --- | --- | --- |
| 1-1 | 115 | 0.8423 |
| 1-2 | 50 | 0.8423 |
| 1-3 | 30 | 0.8423 |
| 1-4 | 10 | **0.8430** |
| 1-5 | 130 | 0.8423 |

- max_depth=30μ—μ„ ROC-AUCκ°€ 0.8430μΌλ΅ λ―Έμ„Έν• μ„±λ¥ ν–¥μƒμ„ λ³΄μΈλ‹¤.
- max_depth=10~30 λ²”μ„μ—μ„λ„ μ¶©λ¶„ν• ν‘ν„λ ¥μ„ ν™•μΈν•  μ μμΌλ©°, λ„λ¬΄ κΉμ€ νΈλ¦¬λ” μ¤νλ ¤ κ³Όμ ν•©μ μ›μΈμ΄ λ  μ μλ‹¤.

<br>

#### μ‹¤ν— 2. num_leaves

|  | num_leaves | ROC AUC |
| --- | --- | --- |
| 2-1 | 38 | 0.8430 |
| 2-2 | 64 | 0.8424 |
| 2-3 | 20 | 0.8420 |
| 2-4 | 32 | 0.8419 |
| 2-5 | 37 | **0.8437** |

- num_leaves=37μ—μ„ ROC-AUCκ°€ 0.8437λ΅ λ―Έμ„Έν• μ„±λ¥ ν–¥μƒμ„ λ³΄μΈλ‹¤.
- num_leaves=37μ΄ κ°€μ¥ λ†’μ€ AUCλ¥Ό κΈ°λ΅ν–μ§€λ§, μ „λ°μ μΈ μ„±λ¥ μ°¨μ΄κ°€ λ―Έμ„Έν•μ—¬ μ‹¤ν— κ²°κ³Όμ ν†µκ³„μ  μ μμ„±μ€ λ‚®μ„ μ μλ‹¤.

<br>

#### μ‹¤ν— 3. subsample

|  | subsample | ROC AUC |
| --- | --- | --- |
| 3-1 | 0.82 | 0.8437 |
| 3-2 | 0.90 | 0.8437 |
| 3-3 | 0.70 | 0.8437 |
| 3-4 | 1.00 | 0.8437 |

- λ¨λ“  μ‹¤ν—μ—μ„ μ„±λ¥ λ™μΌν•¨μ΄ ν™•μΈλμ–΄ subsample=0.82λ¥Ό μ μ§€ν•λ‹¤.

<br>

#### μ‹¤ν— 4. learning_rate

|  | learning_rate | ROC AUC |
| --- | --- | --- |
| 4-1 | 0.0594 | **0.8443** |
| 4-2 | 0.06 | 0.8414 |
| 4-3 | 0.05 | 0.8414 |
| 4-4 | 0.04 | 0.8415 |

- learning_rateλ” Best paramsμΈ 0.0594μ—μ„ κ°€μ¥ λ†’μ€ μ„±λ¥μ„ λ³΄μΈλ‹¤.

<br>

#### μµμΆ… ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•© (LightGBM)

```python
LGBMClassifier(
    n_estimators=500,
    learning_rate=0.0594,
    max_depth=30,
    num_leaves=37,
    min_child_samples=64,
    subsample=0.82
)

```

- μµμΆ… ROC AUC: 0.8443

<br>

## κ²°λ΅ 

| Model | ROC AUC | μ£Όμ” μ„¤μ • |
| --- | --- | --- |
| XGBoost | 0.8471 | max_depth=6, learning_rate=0.06, colsample_bytree=0.65 |
| LightGBM | 0.8443 | max_depth=30, learning_rate=0.0594, num_leaves=37 |
- XGBoostκ°€ μ „μ²΄μ μΌλ΅ λ―Έμ„Έν•κ² λ” λ†’μ€ μ„±λ¥μ„ λ³΄μ—¬μ¤€λ‹¤.
- κ° μ‹¤ν—μ„ ν†µν•΄ μ„±λ¥μ— λ―Όκ°ν• ν•μ΄νΌνλΌλ―Έν„°λ¥Ό κµ¬λ¶„ν•κ³ , λ¶ν•„μ”ν• λ³µμ΅λ„λ‚ κ³Όμ ν•©μ„ μ λ°ν•λ” μ„¤μ •μ„ λ°°μ ν•¨μΌλ΅μ¨ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¬ μ μλ‹¤.

<br>
<br>

## μΈμ‚¬μ΄νΈ λ° νκ³ 

μ΄λ² μ‹¤ν—μ„ ν†µν•΄ ν•μ΄νΌ νλΌλ―Έν„°κ°€ λ¨λΈ μ„±λ¥μ— λ―ΈμΉλ” μν–¥κ³Ό κ·Έ λ―Όκ°λ„λ¥Ό ν™•μΈν•  μ μμ—λ‹¤.

XGBoost μ‹¤ν—μ—μ„λ” max_depth, min_child_weight, colsample_bytree, learning_rateμ™€ κ°™μ€ μ£Όμ” νλΌλ―Έν„°λ¥Ό λ³€κ²½ν•  λ•λ§λ‹¤ ROC AUC μ μκ°€ λ―Έμ„Έν•κ² λ³€λ™λμ—κ³ , μ΄ μ¤‘ learning_rate=0.06 μ΅°ν•©μ΄ κ°€μ¥ λ†’μ€ μ„±λ¥(0.8471)μ„ λ³΄μ—¬μ£Όμ—λ‹¤. colsample_bytreeμ™€ κ°™μ΄ μƒλ€μ μΌλ΅ κ°„κ³Όν•κΈ° μ‰¬μ΄ νλΌλ―Έν„°λ„ μ„±λ¥ ν–¥μƒμ— μ μλ―Έν• μν–¥μ„ μ¤„ μ μμμ„ ν™•μΈν•μ€λ‹¤.

LightGBM μ‹¤ν—μ—μ„λ” HyperOptκ°€ λ„μ¶ν• max_depth=115κ°€ κ³Όλ„ν• λ³µμ΅λ„λ΅ μΈν•΄ μ¤νλ ¤ μ„±λ¥ κ°μ„ μ— λ„μ›€μ΄ λμ§€ μ•μ•κ³ , max_depth=30 μμ¤€μ—μ„ λ” μ•μ •μ μΈ μ„±λ¥μ„ λ³΄μ—¬μ£Όμ—λ‹¤. μ΄λ¥Ό ν†µν•΄ LightGBMμ΄ λΉ λ¥΄κ³  ν¨μ¨μ μΈ κµ¬μ΅°λ¥Ό κ°–κ³  μκΈ° λ•λ¬Έμ—, μ μ ν• κΉμ΄μ™€ λ¦¬ν”„ μ μ΅°μ μ΄ μ„±λ¥κ³Ό ν•™μµ μ‹κ°„ λ¨λ‘μ— κΈμ •μ μΈ μν–¥μ„ μ¤€λ‹¤λ” κ²ƒμ„ ν™•μΈν•  μ μμ—λ‹¤.

μ‹¤ν— μ¤‘ subsample, num_leaves, learning_rate λ“±μ€ μΌμ • λ²”μ„ λ‚΄μ—μ„λ” μ„±λ¥μ— ν° μν–¥μ„ μ£Όμ§€ μ•λ” κµ¬κ°„μ΄ μ΅΄μ¬ν•λ‹¤λ” μ μ„ μ• μ μμ—λ‹¤. μ¶”ν›„ λ¨λΈ νλ‹ μ‹μ—λ” λ―Όκ°λ„κ°€ λ†’μ€ νλΌλ―Έν„°λ¶€ν„° μ°μ„  μ΅°μ •ν•λ” κ²ƒμ΄ ν¨μ¨μ μ΄κ² λ‹¤λ” μƒκ°μ„ ν•  μ μμ—λ‹¤.

μ‹¤ν— κ²°κ³Όμ™€λ” κ΄€λ ¨ μ—†λ” μ΄μ•ΌκΈ°μ§€λ§, λ΅κ·Έ κ΄€λ¦¬(verbosity, early stopping, log_evaluation λ“±)μ™€ GPU ν™μ©μ„ ν†µν•΄ λ¨λΈ ν•™μµ μ‹κ°„μ„ μ¤„μ΄κ³  λ¶ν•„μ”ν• μ¶λ ¥ λ΅κ·Έλ¥Ό μ κ±°ν•λ” λ°©λ²•μ„ μ°Ύμ•„λ³΄λ©° ν¨μ¨μ μΌλ΅ λ¨λΈλ§ν•λ” λ°©λ²•μ„ μ •λ¦¬ν•΄κ°€λ” μ‹κ°„μ΄ λμ—λ‹¤.


<br>

>λ³Έ μ‹¤ν—μ€ μ±… *γ€νμ΄μ¬ λ¨Έμ‹ λ¬λ‹ μ™„λ²½κ°€μ΄λ“γ€* λ¥Ό μ°Έκ³ ν•λ©° μ§„ν–‰ν•μ€μΌλ©°, μ±…μ—μ„ μ μ•λ ν•μ΄νΌνλΌλ―Έν„° νλ‹ μμ„λ¥Ό λ”°λ¥΄λ μ‹¤ν— κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ μ–΄λ–¤ μ΅°ν•©μ΄ μ‹¤μ  μ„±λ¥μ— μ–΄λ–¤ μν–¥μ„ μ£Όλ”μ§€λ¥Ό μ‹¤ν—ν•λ” λ° μ¤‘μ μ„ λ‘μ—μµλ‹λ‹¤.

<br>
